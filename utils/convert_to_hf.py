import os
import argparse
import torch
import json
from omegaconf import OmegaConf
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==========================================
# ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ (ë™ì¼)
# ==========================================

def _load_state_dict_from_file(file_path):
    print(f"ğŸ“‚ Loading raw weights from: {file_path}")
    state_dict = torch.load(file_path, map_location="cpu")
    if "state_dict" in state_dict: return state_dict["state_dict"]
    elif "model" in state_dict: return state_dict["model"]
    elif "state" in state_dict: return state_dict["state"]
    return state_dict

def _clean_state_dict_keys(state_dict):
    print("ğŸ§¹ Cleaning and De-quantizing weights (Int8 -> BF16)...")
    new_state_dict = {}
    
    # 1. SCB(Scale) ê°’ë“¤ì„ ë¨¼ì € ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ì— ë¡œë“œ)
    scb_dict = {}
    for k, v in state_dict.items():
        clean_k = k.replace("_orig_mod.", "").replace("module.", "")
        if "SCB" in clean_k:
            # SCB í‚¤ ì €ì¥: 'model...weight' í˜•íƒœë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ ë³€í™˜
            target_weight_key = clean_k.replace(".SCB", ".weight")
            scb_dict[target_weight_key] = v

    # 2. ê°€ì¤‘ì¹˜ ìˆœíšŒ ë° ë³µêµ¬
    for k, v in state_dict.items():
        new_k = k.replace("_orig_mod.", "").replace("module.", "")
        
        # ë©”íƒ€ë°ì´í„° í‚¤ëŠ” ì €ì¥ ì•ˆ í•¨
        if any(bad in new_k for bad in ["SCB", "weight_format", "_scale", "_amax", "_extra_state"]):
            continue

        if torch.is_tensor(v):
            # (A) Int8 í…ì„œì¸ ê²½ìš° -> ë³µêµ¬(De-quantization) ìˆ˜í–‰
            if v.dtype == torch.int8 or v.dtype == torch.uint8:
                # ê³„ì‚°ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
                v = v.to(device="cpu", dtype=torch.float32)
                
                # ì§ê¿ SCBê°€ ìˆëŠ”ì§€ í™•ì¸
                if new_k in scb_dict:
                    scale = scb_dict[new_k].to(device="cpu", dtype=torch.float32)
                    
                    # [ê³µì‹ ì ìš©] Real = (Int8 / 127.0) * SCB
                    # SCB Shapeì´ (Out_dim,) ì´ë¯€ë¡œ Broadcastingì„ ìœ„í•´ reshape í•„ìš”
                    if scale.ndim == 1 and v.ndim == 2:
                        if scale.shape[0] == v.shape[0]:
                            scale = scale.view(-1, 1) # (N) -> (N, 1)
                        elif scale.shape[0] == v.shape[1]:
                             # í˜¹ì‹œë‚˜ Transposeëœ ê²½ìš°
                             scale = scale.view(1, -1)
                    
                    v = (v / 127.0) * scale
                else:
                    # SCBê°€ ì—†ëŠ”ë° Int8ì´ë‹¤? (ë§¤ìš° ë“œë¬¸ ì¼€ì´ìŠ¤, ê·¸ëƒ¥ í˜•ë³€í™˜)
                    print(f"âš ï¸ Warning: Int8 weight found without SCB: {new_k}")
            
            else:
                # ì´ë¯¸ FP16/BF16/FP32ì¸ ê²½ìš° (LayerNorm ë“±) -> ê·¸ëƒ¥ BF16ìœ¼ë¡œ ë§ì¶¤
                v = v.to(device="cpu", dtype=torch.bfloat16)

            # ìµœì¢… ì €ì¥ í¬ë§·ì€ bfloat16
            v = v.to(dtype=torch.bfloat16)
            
        new_state_dict[new_k] = v
        
    return new_state_dict

def _save_as_lora(config, state_dict, output_dir):
    # (ë³€í™˜ ë¡œì§ì€ .binìœ¼ë¡œ ì €ì¥í•˜ë„ë¡ ìœ ì§€ - safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ìµœì†Œí™”)
    adapter_dir = os.path.join(output_dir, "adapter")
    os.makedirs(adapter_dir, exist_ok=True)
    
    lora_state_dict = {}
    for k, v in state_dict.items():
        if "lora" in k:
            clean_k = k.replace("base_model.model.", "")
            lora_state_dict[clean_k] = v
            
    if not lora_state_dict:
        print("âš ï¸ Warning: No 'lora' keys found in policy.pt")
        return None

    print(f"   > Saving adapter_model.bin to {adapter_dir}...")
    torch.save(lora_state_dict, os.path.join(adapter_dir, "adapter_model.bin"))
    
    peft_config = {
        "peft_type": "LORA",
        "task_type": "CAUSAL_LM",
        "r": config.lora.get("r", 8),
        "lora_alpha": config.lora.get("lora_alpha", 16),
        "lora_dropout": config.lora.get("lora_dropout", 0.05),
        "target_modules": list(config.lora.get("target_modules", ["q_proj", "v_proj"])),
        "base_model_name_or_path": config.model.name_or_path
    }
    with open(os.path.join(adapter_dir, "adapter_config.json"), "w") as f:
        json.dump(peft_config, f, indent=2)
        
    print(f"âœ… LoRA adapter prepared at: {adapter_dir}")
    return adapter_dir

def _save_as_full_model(config, state_dict, output_dir):
    save_path = os.path.join(output_dir, "merged_model")
    
    # [ê¸°ì¡´ ë¡œì§] ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
    has_bin = os.path.exists(os.path.join(save_path, "pytorch_model.bin"))
    has_safe = os.path.exists(os.path.join(save_path, "model.safetensors"))
    is_sharded = os.path.exists(os.path.join(save_path, "pytorch_model.bin.index.json")) or \
                 os.path.exists(os.path.join(save_path, "model.safetensors.index.json"))

    if (has_bin or has_safe or is_sharded) and os.path.exists(os.path.join(save_path, "config.json")):
        print(f"âœ… Merged model already exists at: {save_path}")
        # â˜… ë””ë²„ê¹…ì„ ìœ„í•´: ì´ë¯¸ ìˆì–´ë„ ë¬¸ì œê°€ ìˆë‹¤ë©´ ì§€ìš°ê³  ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ í•˜ë¯€ë¡œ
        # í™•ì‹¤ì¹˜ ì•Šìœ¼ë©´ ì´ í´ë”ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œí•˜ê³  ëŒë¦¬ì„¸ìš”.
        return save_path

    print("ğŸ›  Converting policy.pt to Full Merged Model...")
    base_model_id = config.model.name_or_path
    
    print(f"   > Loading base model: {base_model_id}")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id, 
        torch_dtype=torch.float16, 
        device_map="auto", 
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)

    print("   > Applying weights...")
    cleaned_st = _clean_state_dict_keys(state_dict)
    
    # =========================================================
    # [ìˆ˜ì •] load_state_dictì˜ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ í™•ì¸í•˜ëŠ” ë¡œì§ ì¶”ê°€
    # =========================================================
    load_result = model.load_state_dict(cleaned_st, strict=False)
    
    missing_keys = load_result.missing_keys
    unexpected_keys = load_result.unexpected_keys
    
    # ë¡œê·¸ ì¶œë ¥
    print(f"   > Load Results: {len(missing_keys)} missing, {len(unexpected_keys)} unexpected")
    
    # [ì¤‘ìš”] í•µì‹¬ ê°€ì¤‘ì¹˜ê°€ ë¹ ì¡ŒëŠ”ì§€ í™•ì¸
    # ë³´í†µ rotary_emb.inv_freq ê°™ì€ ë²„í¼ ëª‡ ê°œëŠ” ë¹ ì ¸ë„ ë˜ì§€ë§Œ, 
    # layers, q_proj, v_proj ë“±ì´ ë¹ ì§€ë©´ ì‹¬ê°í•œ ë¬¸ì œì…ë‹ˆë‹¤.
    if len(missing_keys) > 0:
        print(f"âš ï¸  Missing Keys Example (Top 5): {missing_keys[:5]}")
        
        # ë§Œì•½ ì „ì²´ ë ˆì´ì–´ê°€ ë‹¤ ë¹ ì¡Œë‹¤ë©´ ê°•ì œ ì¢…ë£Œí•˜ê±°ë‚˜ ê²½ê³ 
        if any("layers" in k for k in missing_keys):
            print("\n" + "!"*50)
            print("ğŸ˜± CRITICAL ERROR: í•µì‹¬ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   policy.ptì˜ í‚¤ ì´ë¦„ê³¼ ëª¨ë¸ì˜ í‚¤ ì´ë¦„ì´ ë§¤ì¹­ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   _clean_state_dict_keys í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            print("!"*50 + "\n")
            # í•„ìš”ì‹œ raise ValueError("Weight mismatch detected")

    if len(unexpected_keys) > 0:
        print(f"âš ï¸  Unexpected Keys Example (Top 5): {unexpected_keys[:5]}")

    print(f"   > Saving to disk: {save_path}")
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    return save_path

# ==========================================
# [Main Export Function] - ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„
# ==========================================

def prepare_weights_for_vllm(checkpoint_dir):
    checkpoint_dir = os.path.abspath(checkpoint_dir)
    parent_dir = os.path.dirname(checkpoint_dir)
    config_path = os.path.join(parent_dir, "config.yaml")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config not found: {config_path}")
    
    config = OmegaConf.load(config_path)
    
    enable_lora = False
    if "lora" in config and hasattr(config.lora, "enabled"):
        enable_lora = config.lora.enabled

    # 1. LoRAì¸ ê²½ìš°
    if enable_lora:
        adapter_path = os.path.join(checkpoint_dir, "adapter")
        
        # [ìˆ˜ì •] .bin ë¿ë§Œ ì•„ë‹ˆë¼ .safetensorsë„ í™•ì¸í•©ë‹ˆë‹¤!
        has_bin = os.path.exists(os.path.join(adapter_path, "adapter_model.bin"))
        has_safetensors = os.path.exists(os.path.join(adapter_path, "adapter_model.safetensors"))
        
        # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë³€í™˜ ì‹œë„
        if not (has_bin or has_safetensors):
            print("âš ï¸ No adapter file found (bin/safetensors). Looking for policy.pt...")
            policy_path = os.path.join(checkpoint_dir, "policy.pt")
            if os.path.exists(policy_path):
                state_dict = _load_state_dict_from_file(policy_path)
                _save_as_lora(config, state_dict, checkpoint_dir)
            else:
                print("âŒ policy.pt not found either. Using base model ONLY.")
                return config.model.name_or_path, False, None
        else:
            print(f"âœ… Found existing adapter (Safetensors/Bin) at: {adapter_path}")
        
        return config.model.name_or_path, True, adapter_path

    # 2. Full Fine-tuningì¸ ê²½ìš°
    else:
        merged_path = os.path.join(checkpoint_dir, "merged_model")
        
        # [ìˆ˜ì •] ë³‘í•© ëª¨ë¸ ë‚´ë¶€ë„ safetensors ë“± ë‹¤ì–‘í•˜ê²Œ í™•ì¸
        # ê°„ë‹¨íˆ config.jsonê³¼ ëª¨ë¸ íŒŒì¼ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
        exists = os.path.exists(merged_path) and os.path.exists(os.path.join(merged_path, "config.json"))
        
        if not exists:
            policy_path = os.path.join(checkpoint_dir, "policy.pt")
            if os.path.exists(policy_path):
                state_dict = _load_state_dict_from_file(policy_path)
                _save_as_full_model(config, state_dict, checkpoint_dir)
            else:
                print("âš ï¸ policy.pt not found. Using raw base model.")
                return config.model.name_or_path, False, None
                
        return merged_path, False, None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint_dir", type=str, required=True)
    args = parser.parse_args()
    
    m, l, p = prepare_weights_for_vllm(args.checkpoint_dir)
    print(f"\nResult:\nBase Model: {m}\nUse LoRA: {l}\nAdapter Path: {p}")